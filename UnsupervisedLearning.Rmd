---
title: "Unsupervised Learning - Strokes"
author: "Lorenzo Rossi"
date: "31/8/2021"
abstract: I'll use unsupervised learning methods to investigate what are the main factors that contribute to the risk of having a stroke through different variables, that I considered being reelvant 
output:
  html_document: default
  pdf_document: default
---

##Problem Understanding

Since it's not possible to predict them in an accurate way, the starting point of the research was to investigate the main factors that are associated (or that influence in some way) with strokes.
I'll use two main methods in order to analyse the variables: Principal Component Analysis and K-Means clustering.
The aim is to get an idea is to understand what are the elements that are most recurrent, or that can explain in some way, the onset of a heart attack.

#Data

The dataset was retrieved from Kaggle's Stroke Prediction Dataset <https://www.kaggle.com/fedesoriano/stroke-prediction-dataset>. The original dataset contained thousands of cases. I kept a sample of 1000.
The variables are:
- id: id of the patient
- stroke: 0 = no stroke; 1 = the subject had a stroke
- Bmi: bmi index of the person
- age: age of the person
- worktype: type of job of the subject: 1 = no job; 2 = public emplyee; 3 = private employee; 4 = self-emploied
- smoking status: 0 = non smoker; 0,5 = formerly smoker; 1 = smoker.
- avg_glucose_level: the glucose level of the subject
- married: 0 = no; 1 = yes
- location: where the subject lives: 0 = countryside; 1 = city
- hypertension: 0 = no hypertension; 1 = the patient has hypertension
- heart_disease: 0 = no heart disease; 1 = the patient has heart disease

##Theoretical Background
In the unsupervised learning only independent variables are observed and there is no interest in prediction because there isn't a response variable.
Therefore, the aim of such instruments is to find interesting ways to visualize data and to discover subgroups among the variables and observations. 
I'll discussed two methods during the analysis:
- **Principal Component Analysis (PCA)**
- **K-Means clustering**

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, include=FALSE}
library(car)
library(readr)
library(dplyr)
library(tidyr)
library(readxl)
library(ggpubr)
library(ggplot2)
library(Hmisc)
library(olsrr)
library(tidyverse)
library(caret)
library(Metrics)
library(corrplot)
library(glmnet)
library(leaps)
library(class)
library(selectiveInference)
library(RCurl)
library(tree)
library(ISLR)
library(plot3D)
library(sjPlot)
library(plotly)
library(haven)
library(corrplot)
library(plyr)
library(PerformanceAnalytics)
library(gbm)
library(randomForest)
library(FactoMineR)
library(factoextra)
library(cluster)
library(gridExtra)
```

```{r pressure, echo=FALSE}
df <- read_excel("stroke.xlsx")
dataset <- data.frame(df, row.names = 1)
dataset <- dataset[,2:11]
dataset <- scale(dataset)
```
##Data exploration

Since the main variable is cathegorical, there's not much to analyse, nor computing QQ-plots or boxplots. We can however see the distribution of the other numerical variables.

```{r pressure, echo=FALSE}
summary(df$stroke)

hist(df$age)
hist(df$BMI)
hist(df$worktype)
hist(df$avg_glucose_level)
```

### Principal Component Analysis

The PCA is a technique which gives us a low dimensional representation of the dataset, exploiting covariance matrix of the starting dataset. The aim is to find a sequence of combination of variables that have maximal variance and are uncorrelated. 
For this reason, the process id developed sequentially. First, it’s needed to find the first principal component of a set of features: this is that normalized linear combination of features which has the largest variance, where the sum of squared coefficients is equal to 1 and the set of the coefficients is called the principal component loading vector. Then, the first principal component is computed starting from this vector, using singular value decomposition and with the aim to get as much variance as possible. 
In other words, we want to identify the direction among the feature space where the data vary the most and that is more relevant to explain how data are spread in the our space. The PCA is useful for visualization because we can project data along this dimension to understand a good part of data distribution. 
The representation can be done in two dimensions: for this reason, it’s relevant do determine the second principal component. This is that linear combination of features that has the largest variance and, at the same time, is uncorrelated with the first principal component. Given this last assumption, second component turns out to lay on the orthogonal direction of the first component and this is a great feature to visualize the data in a two dimensional space. 
Principal components can be more than two but not more the number of observations minus one or number of features and all of them need to be orthogonal to the previous component. 
Principal components are also used to get portion of the total variance present in a dataset and this is the rule used to select the best number of principal components. The idea is to add them to the dataset as new variables until the next one improves significantly the explained variance of our observations. 
A general rule is based on the use of the singular values associated with each component, which recommends to add components until they have values greater than a threshold, usually 1, because it would mean they are able to explain the spread of the data as much as more than one variable of the original dataset.


#Normalization
```{r pressure, echo=FALSE}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x))) }

dataset <- scale(dataset)

n <- nrow(dataset)
p <- ncol(dataset)

M <- colMeans(dataset)
sigma <- apply(dataset,2,sd)
descriptive<-round(cbind(M,sigma),2)
descriptive
```

We proceed by extracting the correlation matrix from the dataset and then its eigenvalues. 
As the underlying table shows, there is a first principal component that explains almost 20% of the variance. 
There are other three components deriving from an eigenvalue just above one.

```{r pressure, echo=FALSE}
rho <- cor(dataset)

autoval <- eigen(rho)$values
autovec <- eigen(rho)$vectors

pvarsp = autoval/p
pvarspcum = cumsum(pvarsp)
pvarsp

tab<-round(cbind(autoval,pvarsp*100,pvarspcum*100),3)
colnames(tab)<-c("eigenval","%var","%cumvar")
tab
```

An easy way to select components is a scree plot.
Since the table witnesses the presence of a first principal component followed by three other ones lying an order of magnitude below, I prefer to avoid the elbow rule and instead select every component above one.

```{r pressure, echo=FALSE}
plot(autoval,type="b",main="Scree Diagram", xlab="Number of Component", ylab="Eigenvalues")
abline(h=1,lwd=3,col="red")

eigen(rho)$vectors[,1:4]
```
PCA is an appropriate tool for data visualization. In this case, there is a tradeoff between visualization and reliability of the model. On one hand, the fifth component has slightly significant explanatory importance since the eigenvalue is just below 1 and it moves the variance of almost 10%. 
On the other hand, it is still below the threshold and could be neglected without a significant loss. In order to have a higher explained variance one would select a five-dimensional model, while a lower-dimensionality fashion for plotting requires to have at most three components.

#we investigate what is explained by the components
```{r pressure, echo=FALSE}
comp<-round(cbind(
  -eigen(rho)$vectors[,1]*sqrt(autoval[1]),
  -eigen(rho)$vectors[,2]*sqrt(autoval[2]),
  -eigen(rho)$vectors[,3]*sqrt(autoval[3]),
  -eigen(rho)$vectors[,4]*sqrt(autoval[4])
),3)

rownames(comp)<-row.names(descriptive)
colnames(comp)<-c("comp1","comp2","comp3","comp4")
comp


communality<-comp[,1]^2+comp[,2]^2+comp[,3]^2+comp[,4]^2
comp<-cbind(comp,communality)
comp
```
From the results we see that the four principal components that affect strokes are: age, smoking status, bmi and type of job.

However, as explained above, we need to remove the last one in order to plot.
Since in this analysis PCA is used for description and visualization, we focus on a three components analysis. 
The three component analysis is the most useful one to read loadings and thus understand how variables are explained by the principal components. 
First of all, we select the corresponding eigenvectors.

```{r pressure, echo=FALSE}
#part 2 - three components
eigen(rho)$vectors[,1:3]

#we investigate what is explained by the components
comp<-round(cbind(
  -eigen(rho)$vectors[,1]*sqrt(autoval[1]),
  -eigen(rho)$vectors[,2]*sqrt(autoval[2]),
  -eigen(rho)$vectors[,3]*sqrt(autoval[3])
),3)

rownames(comp)<-row.names(descriptive)

colnames(comp)<-c("comp1","comp2","comp3")
communality<-comp[,1]^2+comp[,2]^2+comp[,3]^2
comp<-cbind(comp,communality)
comp
```

We proceed generating the scores for each variable in relation with the three components chosen.

We compute also the score for every patient. In this case, each group of three scores can be intepreted as a coordinate in the following tridimensional plot.

```{r pressure, echo=FALSE}
score <- dataset%*%autovec[,1:3]
round(score,3)


#score plot
scorez<-round(cbind
              (-score[,1]/sqrt(autoval[1]),
                score[,2]/sqrt(autoval[2]),
                score[,3]/sqrt(autoval[3])),2)

x <- scorez[,1]
y <- scorez[,2]
z <- scorez[,3]
```
The plot

```{r pressure, echo=FALSE}
scatter3D(x, y, z, colvar = dataset[,1],
          xlab = "comp1", ylab = "comp2", zlab = "comp3",
          main = "PCA - 4 components",
          bty = "g", ticktype = "detailed",
          theta = 15, phi = 20, col = gg.col(100),
          clab = "Stroke", d = 2, type = "h",
          pch = 19, cex = 0.5)

compz<-round(cbind
             (-comp[,1]/sqrt(autoval[1]),
               comp[,2]/sqrt(autoval[2]),
               comp[,3]/sqrt(autoval[3])),2)
x <- compz[,1]
y <- compz[,2]
z <- compz[,3]

scatter3D(x,y,z,
          col = "red",add = TRUE,
          pch = 19, cex = 0.8)
```
Despite the grphical representation, the high number of instances makes difficult the understanding of the plot.
However, there's an alternative way to represent the variables: using the prcomp() function and the factoextra package.

```{r pressure, echo=FALSE}
dt2 <- prcomp(dataset)

fviz_eig(dt2)

fviz_pca_var(dt2,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = F     # Avoid text overlapping
)

```
We can see the graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.


### Clustering

The aim of the clustering is different respect to PCA: here, we want to detect if subgroups or clusters are present in the dataset, seeking a data partition such that observations within the same group are *similar*. 
The concept of similarity becomes central with clustering and it depends on type of variables, which need to be numeric, and on the type of distance we are considering.
The algorithms can be several, but there is a main difference between two groups: 
- K-means, where the number of clusters is preselected, but an optimal choice can be made through testing.
- hierarchical clustering, where the clustering ends up with a tree-like visual representation, called dendrogram.

My analysis if focused on K-Means clustering and its different algorithms based on different assumptions, with the research of an optimal k that would lead to the best interpretations.


We srart from the previous scaled dataset to choose a pre-selected number of clusters

```{r pressure, echo=FALSE}
k2 <- kmeans(dataset, centers = 2, nstart = 25)
str(k2)

fviz_cluster(k2, data = dataset)
```
We can choose any number of cluster that we want

```{r pressure, echo=FALSE}
k3 <- kmeans(dataset, centers = 3, nstart = 25)
k4 <- kmeans(dataset, centers = 4, nstart = 25)
k5 <- kmeans(dataset, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = dataset) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point", data = dataset) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point", data = dataset) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point", data = dataset) + ggtitle("k = 5")

grid.arrange(p1, p2, p3, p4, nrow = 2)
```

The basic idea behind cluster methods, such as k-means clustering, is to define clusters such that the total intra-cluster variation is minimized.
The total within-cluster sum of square (wss) measures the compactness of the clustering and we want it to be as small as possible. Thus, we can use the following algorithm to define the optimal clusters:
1. Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 10 clusters
2. For each k, calculate the total within-cluster sum of square (wss)
3. Plot the curve of wss according to the number of clusters k.
4. The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.

We can implement this in R with the following code. 

```{r pressure, echo=FALSE}
set.seed(123)

wss <- function(k) {
  kmeans(dataset, k, nstart = 10 )$tot.withinss
}

k.values <- 1:15

set.seed(123)
fviz_nbclust(df, kmeans, method = "wss")

#2-3 optimal kluster
```
The average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. 
A high average silhouette width indicates a good clustering. The average silhouette method computes the average silhouette of observations for different values of k. 
The optimal number of clusters k is the one that maximizes the average silhouette over a range of possible values for k.

```{r pressure, echo=FALSE}
fviz_nbclust(df, kmeans, method = "silhouette") #silhouette
```

We plot tha final representation

```{r pressure, echo=FALSE}
set.seed(123)
final <- kmeans(dataset, 2, nstart = 25)

fviz_cluster(final, data = dataset, geom = "point") 
```
From the analysis it seems that there are only two clusters present. It's likely that the two clusters are divided into those who have had a heart attack and those who have not.
The intersection will be composed of those individuals who, despite not having had a heart attack, share certain life factors with those who have had it, i.e. the principal components previously analyzed.

#Conclusion

I computed several analysis to chekc the elements that influenced the most strokes. Starting from the PCA we could see the high correlation with age, smoking status and bmi.
It also seems pretty logical on a medical level.
We saw then, with the help of K-Means clustering, that these elements are shared with other people who never had heart attacks. 
With the exception of age, it's obviou that smoking or having an high bmi may be harmful for health, with the risk of incurring in cardiac problems.

#Appendix

library(car)
library(readr)
library(dplyr)
library(tidyr)
library(readxl)
library(ggpubr)
library(ggplot2)
library(Hmisc)
library(olsrr)
library(tidyverse)
library(caret)
library(Metrics)
library(corrplot)
library(glmnet)
library(leaps)
library(class)
library(selectiveInference)
library(RCurl)
library(tree)
library(ISLR)
library(plot3D)
library(sjPlot)
library(plotly)
library(haven)
library(corrplot)
library(plyr)
library(PerformanceAnalytics)
library(gbm)
library(randomForest)
library(FactoMineR)
library(factoextra)
library(cluster)
library(gridExtra)

df <- read_excel("stroke.xlsx")

summary(df$stroke)

hist(df$age)
hist(df$BMI)
hist(df$worktype)
hist(df$avg_glucose_level)

#pca

df <- read_excel("stroke.xlsx")
dataset <- data.frame(df, row.names = 1)
dataset <- dataset[,2:11]
dataset <- scale(dataset)


#Normalization
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x))) }

dataset <- scale(dataset)

n <- nrow(dataset)
p <- ncol(dataset)

M <- colMeans(dataset)
sigma <- apply(dataset,2,sd)
descriptive<-round(cbind(M,sigma),2)
descriptive

#create correlation matrix
rho <- cor(dataset)

autoval <- eigen(rho)$values
autovec <- eigen(rho)$vectors

pvarsp = autoval/p
pvarspcum = cumsum(pvarsp)
pvarsp

tab<-round(cbind(autoval,pvarsp*100,pvarspcum*100),3)
colnames(tab)<-c("eigenval","%var","%cumvar")
tab

#scree diagram to select the components
plot(autoval,type="b",main="Scree Diagram", xlab="Number of Component", ylab="Eigenvalues")
abline(h=1,lwd=3,col="red")

eigen(rho)$vectors[,1:4]


#we investigate what is explained by the components
comp<-round(cbind(
  -eigen(rho)$vectors[,1]*sqrt(autoval[1]),
  -eigen(rho)$vectors[,2]*sqrt(autoval[2]),
  -eigen(rho)$vectors[,3]*sqrt(autoval[3]),
  -eigen(rho)$vectors[,4]*sqrt(autoval[4])
),3)

rownames(comp)<-row.names(descriptive)
colnames(comp)<-c("comp1","comp2","comp3","comp4")
comp


communality<-comp[,1]^2+comp[,2]^2+comp[,3]^2+comp[,4]^2
comp<-cbind(comp,communality)
comp


#part 2 - three components
eigen(rho)$vectors[,1:3]

#we investigate what is explained by the components
comp<-round(cbind(
  -eigen(rho)$vectors[,1]*sqrt(autoval[1]),
  -eigen(rho)$vectors[,2]*sqrt(autoval[2]),
  -eigen(rho)$vectors[,3]*sqrt(autoval[3])
),3)

rownames(comp)<-row.names(descriptive)

colnames(comp)<-c("comp1","comp2","comp3")
communality<-comp[,1]^2+comp[,2]^2+comp[,3]^2
comp<-cbind(comp,communality)
comp

#calculate the scores for the selected components and graph them
#calculate components for each unit
score <- dataset%*%autovec[,1:3]
round(score,3)


#score plot
scorez<-round(cbind
              (-score[,1]/sqrt(autoval[1]),
                score[,2]/sqrt(autoval[2]),
                score[,3]/sqrt(autoval[3])),2)

x <- scorez[,1]
y <- scorez[,2]
z <- scorez[,3]

#plots
scatter3D(x, y, z, colvar = dataset[,1],
          xlab = "comp1", ylab = "comp2", zlab = "comp3",
          main = "PCA - 4 components",
          bty = "g", ticktype = "detailed",
          theta = 15, phi = 20, col = gg.col(100),
          clab = "Stroke", d = 2, type = "h",
          pch = 19, cex = 0.5)

compz<-round(cbind
             (-comp[,1]/sqrt(autoval[1]),
               comp[,2]/sqrt(autoval[2]),
               comp[,3]/sqrt(autoval[3])),2)
x <- compz[,1]
y <- compz[,2]
z <- compz[,3]

scatter3D(x,y,z,
          col = "red",add = TRUE,
          pch = 19, cex = 0.8)

#alternative way

dt2 <- prcomp(dataset)

fviz_eig(dt2)

fviz_pca_var(dt2,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = F     # Avoid text overlapping
)

#kmeans

#from the previous scaled dataset

k2 <- kmeans(dataset, centers = 2, nstart = 25)
str(k2)

fviz_cluster(k2, data = dataset)

k3 <- kmeans(dataset, centers = 3, nstart = 25)
k4 <- kmeans(dataset, centers = 4, nstart = 25)
k5 <- kmeans(dataset, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = dataset) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point", data = dataset) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point", data = dataset) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point", data = dataset) + ggtitle("k = 5")

grid.arrange(p1, p2, p3, p4, nrow = 2)

#finding the best 

set.seed(123)

# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(dataset, k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters

set.seed(123)
fviz_nbclust(df, kmeans, method = "wss")

#2-3 optimal kluster

fviz_nbclust(df, kmeans, method = "silhouette") #silhouette

set.seed(123)
final <- kmeans(dataset, 2, nstart = 25)
final2 <- kmeans(dataset, 3, nstart = 25)

f1 <- fviz_cluster(final, data = dataset, geom = "point") 
f2 <- fviz_cluster(final2, data = dataset, geom = "point")

grid.arrange(f1, f2, nrow = 2)

